{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import shutil\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_path = \"/root/workspace/reny/petmriBrain/code/3nnUNet-master/output/nnUNet_trained_models/nnUNet/3d_fullres/Task200_mriBrain/nnUNetTrainerV2__nnUNetPlansv2.1/fold_0/Task200_best_infer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "print (s.keys())\n",
    "dict_keys(['author', 'description', 'id', 'name', 'results', 'task', 'timestamp'])\n",
    "\n",
    "print (s['results'].keys())\n",
    "dict_keys(['all', 'mean'])\n",
    "\n",
    "print (len(s['results']['all']))\n",
    "60\n",
    "\n",
    ">>> print (summary['results']['all'][0].keys())\n",
    "dict_keys(['1', '10', '11', '12', '13', '14', '15', .... '44', 'reference', 'test'])\n",
    "\n",
    ">>> print (summary['results']['all'][0]['1'])\n",
    "{'Accuracy': 0.9994197487831116, 'Dice': 0.9773196933091348, 'False Discovery Rate': 0.024405445758699074, 'False Negative Rate': 0.020949055705443587, 'False Omission Rate': 0.0002709781168868454, 'False Positive Rate': 0.00031679078079061007, 'Jaccard': 0.9556453633799736, 'Negative Predictive Value': 0.9997290218831132, 'Precision': 0.9755945542413009, 'Recall': 0.9790509442945564, 'Total Positives Reference': 214234, 'Total Positives Test': 214993, 'True Negative Rate': 0.9996832092192094}\n",
    "\n",
    "\n",
    ">>> print (len(summary['results']['mean']))\n",
    "44\n",
    "\n",
    ">>> print (summary['results']['mean'].keys())\n",
    "dict_keys(['1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '4', '40', '41', '42', '43', '44', '5', '6', '7', '8', '9'])\n",
    ">>> print (summary['results']['mean']['1'])\n",
    "{'Accuracy': 0.9991692612568538, 'Dice': 0.9665738308168504, 'False Discovery Rate': 0.03496317886602274, 'False Negative Rate': 0.031477456209323386, 'False Omission Rate': 0.00039192513639064574, 'False Positive Rate': 0.00044945874675844286, 'Jaccard': 0.9358444684083161, 'Negative Predictive Value': 0.9996080748636093, 'Precision': 0.9650368211339772, 'Recall': 0.9685225437906766, 'Total Positives Reference': 213490.28333333333, 'Total Positives Test': 214443.33333333334, 'True Negative Rate': 0.9995505412532417}\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv(csv_rows,csv_path,name_attribute):\n",
    "    with open(csv_path, mode='w') as file:\n",
    "        writerCSV = pd.DataFrame(columns=name_attribute, data=csv_rows)\n",
    "        writerCSV.to_csv(csv_path, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(pred_path, \"summary.json\")) as f:\n",
    "    summary = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_singlecase_csv(flag,summary_info):\n",
    " \n",
    "    #flag = [\"Accuracy\",\"Dice\",\"False Negative Rate\",\"False Positive Rate\",\"Jaccard\",\n",
    "    #\"Precision\",\"Recall\",\"True Negative Rate\"]\n",
    "    \n",
    "    csv_list = []\n",
    "    for i,dict_keys in summary_info.items():\n",
    "        img = dict()\n",
    "        img['name'] = value['reference'].split('/')[-1][:-7]\n",
    "        for i in range(1,45):\n",
    "            img[str(i)] = value[str(i)][flag]\n",
    "        csv_list.append(img)\n",
    "    \n",
    "    return csv_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_label_csv(summary_info):\n",
    " \n",
    "    flags = [\"Accuracy\",\"Dice\",\"False Negative Rate\",\"False Positive Rate\",\"Jaccard\",\"Precision\",\"Recall\",\"True Negative Rate\"]\n",
    "    \n",
    "    csv_list = []\n",
    "    for i,val in summary.items():\n",
    "        img = dict()\n",
    "        img['name'] = str(i)\n",
    "        for fg in flags:\n",
    "            img[fg] = val[fg]\n",
    "        csv_list.append(img)\n",
    "    return csv_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_attribute = ['id']\n",
    "for i in range(1,45):\n",
    "    name_attribute.append(str(i))\n",
    "\n",
    "flags = [\"Accuracy\",\"Dice\",\"False Negative Rate\",\"False Positive Rate\",\"Jaccard\",\"Precision\",\"Recall\",\"True Negative Rate\"]\n",
    "for flag_name in flags:\n",
    "    evaluate_list = generate_datainfo_csv(flag_name,summary['results']['all'])\n",
    "    save_csv(evaluate_list,os.path.join(pred_path,flag_name+'.csv'),name_attribute)\n",
    "    \n",
    "label_list = generate_label_csv(summary['results']['mean'])\n",
    "save_csv(label_list,os.path.join(pred_path,'labels.csv'),['label']+flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
